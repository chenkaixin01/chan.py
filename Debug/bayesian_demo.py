import xgboost as xgb
from bayes_opt import BayesianOptimization
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score

# åŠ è½½ç¤ºä¾‹æ•°æ®
dtrain = xgb.DMatrix("T1_buy_train.libsvm?format=libsvm")  # load sample
dval = xgb.DMatrix("T1_buy_val.libsvm?format=libsvm")  # load sample
dtest = xgb.DMatrix("T1_buy_test.libsvm?format=libsvm")  # load sample

# å®šä¹‰ç›®æ ‡å‡½æ•°
def xgb_evaluate(max_depth,eta, gamma, colsample_bytree, learning_rate, min_child_weight,scale_pos_weight,subsample,reg_lambda,reg_alpha):
    params = {
        "objective": "binary:logistic",
        "eval_metric": "auc",
        "max_depth": int(max_depth),
        "eta": eta,
        "gamma": gamma,
        "colsample_bytree": colsample_bytree,
        "learning_rate": learning_rate,
        "min_child_weight": min_child_weight,
        "scale_pos_weight": scale_pos_weight,
        "subsample": subsample,
        "reg_lambda": reg_lambda,
        "reg_alpha": reg_alpha,
        "verbosity": 0
    }
    # model = xgb.train(
    #     params,
    #     dtrain,
    #     num_boost_round=100,
    #     evals=[(dtrain, "train"), (dval, "eval")],
    #     early_stopping_rounds=50,
    #     verbose_eval=False
    # )
    # preds = model.predict(dval)
    # auc = roc_auc_score(dval.get_label(), preds)
    # return auc
    cv_result = xgb.cv(
        params,
        dtrain,
        num_boost_round=100,
        nfold=10,
        stratified=True,
        early_stopping_rounds=50,
        seed=42,
        verbose_eval=False
    )
    return cv_result["test-auc-mean"].max()


# è®¾ç½®è¶…å‚æ•°ç©ºé—´
param_bounds = {
    "max_depth": (1, 10),
    "eta": (0.1, 1),
    "gamma": (0, 5),
    "colsample_bytree": (0.3, 1),
    "learning_rate": (0.01, 0.3),
    "min_child_weight": (1, 10),
    "scale_pos_weight": (1, 5),
    "subsample": (0.5, 1),
    "reg_lambda": (0, 5),
    "reg_alpha": (0, 5),
}
# è´å¶æ–¯ä¼˜åŒ–å™¨
optimizer = BayesianOptimization(
    f=xgb_evaluate,
    pbounds=param_bounds,
    random_state=42,
    verbose=2
)

# å¼€å§‹ä¼˜åŒ–
optimizer.maximize(
    init_points=10,  # éšæœºæ¢ç´¢ç‚¹
    n_iter=250        # è´å¶æ–¯ä¼˜åŒ–è¿­ä»£æ¬¡æ•°
)

# è¾“å‡ºæœ€ä¼˜ç»“æœ
# print("Best result:", optimizer.max)

# è·å–æœ€ä¼˜è¶…å‚æ•°
best_params = optimizer.max['params']
best_params['max_depth'] = int(best_params['max_depth'])
best_params['min_child_weight'] = int(best_params['min_child_weight'])

# ğŸ‘‡ è·å–æœ€ä½³è½®æ•°ï¼ˆç”¨ xgb.cv å†è·‘ä¸€æ¬¡ï¼Œä¸ºäº†æ‹¿åˆ° best_iterationï¼‰
cv_result = xgb.cv(
    best_params,
    dtrain,
    num_boost_round=200,
    nfold=5,
    stratified=True,
    early_stopping_rounds=50,
    seed=42,
    verbose_eval=False
)
best_num_round = cv_result.shape[0]  # æœ€ä¼˜è½®æ•° = best_iteration
print("Best params: ", optimizer.max)
print("Best num_round: ", best_num_round)
